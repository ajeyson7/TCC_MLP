# -*- coding: utf-8 -*-
"""MLP_TCC_JEYSON.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mrMBwWESURZza1R7DsVeb1daFU4Nb4Rt
"""

import pandas                         as pd
import datetime                       as dt
import matplotlib.pyplot              as pyplot
import numpy                          as np
from sklearn.model_selection          import train_test_split
from sklearn.metrics                  import accuracy_score,mean_absolute_error,mean_absolute_percentage_error
import plotly.express as px

from sklearn.neural_network           import MLPClassifier
from sklearn.neighbors                import KNeighborsClassifier
from sklearn.svm                      import SVC
from sklearn.gaussian_process         import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.ensemble                 import GradientBoostingClassifier
from sklearn.tree                     import DecisionTreeClassifier
from sklearn.ensemble                 import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier
from sklearn.naive_bayes              import GaussianNB
from sklearn.discriminant_analysis    import QuadraticDiscriminantAnalysis
from sklearn.linear_model             import SGDClassifier,LogisticRegression,LogisticRegressionCV
from sklearn.metrics                  import accuracy_score,mean_absolute_error,mean_absolute_percentage_error

import pickle

from sklearn.metrics                  import roc_curve,roc_auc_score

import matplotlib.pyplot as plt

import sklearn
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report,accuracy_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
RANDOM_SEED = 42
LABELS = ["Normal", "Fraude"]

# Commented out IPython magic to ensure Python compatibility.
#Basic libraries
import pandas as pd
import numpy as np

#Visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.offline import plot, iplot, init_notebook_mode
init_notebook_mode(connected=True)
# %matplotlib inline

#preprocessing libraries
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

#ML libraries
import tensorflow as tf
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_excel('/content/drive/MyDrive/prd_00294_ocorr_fraudes.xlsx')

data.head(10)

print('Número máximo de valores nulos em qualquer coluna: ' +
str(data.isnull().sum().max()))

data.info()

round(data['FRAUDE'].value_counts(normalize=True).mul(100)).astype(str)

count_fraude = pd.value_counts(data['FRAUDE'], sort = True)

count_fraude.plot(kind = 'bar', rot=0)

plt.title("Distribuição de Transações Fraudulentas")

plt.xticks(range(2), LABELS)

plt.xlabel("FRAUDES")

plt.ylabel("Frequencia")

## Criando variáveis fraude e sem_fraude 

fraude = data[data['FRAUDE']==1]

sem_fraude = data[data['FRAUDE']==0]

print(fraude.shape,sem_fraude.shape)

fraude.VALOR_FRAUDADO.describe()

sem_fraude.VALOR_FRAUDADO.describe()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Valor fraudados por transação e classe')
bins = 50
ax1.hist(fraude.VALOR_FRAUDADO, bins = bins)
ax1.set_title('Fraude')
ax2.hist(sem_fraude.VALOR_FRAUDADO, bins = bins)
ax2.set_title('Sem Fraude')
plt.xlabel('Valores Fraudados (R$)')
plt.ylabel('Numbero de Transações')
plt.xlim((0, 60000))
plt.yscale('log')
plt.show();

"""## **Correção dos dados do DataFrame**

Algumas colunas contendo o tipo "object" devem ser convertidas para dummies(booleanas) ou deletadas, já que não podem pertencer a base de dados para treinamento. Sempre com o objetivo de perder o menor número de informação possível.


"""

data=data.drop(['ID_CONTA_CLIENTE','ID_CONTA_DESTINO','REGIAO','CEP','VALOR_FRAUDADO'],axis=1)

"""**Etapas:**

-> Deletadas variáveis com menor significancia para o modelo

-> Reclassificação da variável 'OPERACAO' para boolean

-> Separação da variável data para 3 novas colunas e drop da original
"""

data=pd.get_dummies(data,columns=['OPERACAO'])

data['ANO_OPERACAO'] = pd.DataFrame(pd.DatetimeIndex(data.DATA_OPERACAO).year)
data['MES_OPERACAO'] = pd.DataFrame(pd.DatetimeIndex(data.DATA_OPERACAO).month)
data['DIA_OPERACAO'] = pd.DataFrame(pd.DatetimeIndex(data.DATA_OPERACAO).day)
data=data.drop(['DATA_OPERACAO'],axis=1)

data.info()

"""Analisando o balanceamento entre os dados, entre fraudes e não fraudes, para verificar a necessidade de sampling-splitting

Utiliza-se a métrica da página: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data

**Segregação dos eixos x e y**

x = Dados sem a coluna 'FRAUDE' y = Coluna 'FRAUDE'
"""

x = data.drop(['FRAUDE'],axis=1)
y = data['FRAUDE']

"""**Spread dos eixos de treino e teste**

Treino com tamanho de 20% e Teste 80%

Orientação pelo eixo y no caso de saídas binárias (0 e 1)

Randomização das camadas para dados mais homogêneos com intervalos de 12
"""

x_train, x_test,y_train,y_test = train_test_split(x,y,train_size=0.2,stratify=y,random_state=12)

"""Dataframe para Machine Learnings"""

from scipy.optimize.linesearch import line_search_BFGS
classifiers = pd.DataFrame({
    'name':
    ["Nearest_Neighbors",
     "Gradient_Boosting",
     "Decision_Tree",
     "Extra_Trees",
     "Random_Forest",
     "Neural_Net",
     "AdaBoost",
     "Naive_Bayes",
     "LogisticRegression"],
  'algorithm' : [
    KNeighborsClassifier(3),
    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),
    DecisionTreeClassifier(max_depth=5),
    ExtraTreesClassifier(n_estimators=10, min_samples_split=2),
    RandomForestClassifier(max_depth=5, n_estimators=100),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(n_estimators=100),
    GaussianNB(),
    LogisticRegression(C=1,)
    ],
    'algorithm_score': '',
    'accuracy':'',
    'auc':'',
    'probs':''
    })

"""**Set dos scores para o Dataframe**"""

def score_loc():
  for index,classifier in classifiers.iterrows():
    try:
      algorithm = classifiers.iloc[index]['algorithm']
      score = algorithm.score
      classifiers.score[index] = score
      accuracy = round(score*100,2),'%'
      classifiers.accuracy[index]=accuracy
    except Exception as ex:
      print('fail on catch score: ', ex)

"""**Treinamento dos dados**"""

def classifiers_fit():
  for index,classifier in classifiers.iterrows():
    try:
      algorithm = classifiers.iloc[index]['algorithm']
      name =classifiers.iloc[index]['name']
      algorithm.fit(x_train,y_train)
      score=algorithm.score(x_test,y_test)
      classifiers_scores(score,index)
    except Exception as ex:
      print('Error while training:', ex)

"""Funções para Save/Load do modelo do algoritmo"""

def save_model(model,classifier_name):
  with open('/content/drive/MyDrive/pickle_models/'+classifier_name,'wb') as save_file:
    pickle.dump(model,save_file)

def load_model(name):
  with open('/content/drive/MyDrive/pickle_models/'+name,'rb')as f:
    model = pickle.load(f)
  return model

"""**Salvar Modelos para arquivos em Pickle**"""

def classifiers_save():
  for index,classifier in classifiers.iterrows():
    try:
      algorithm = classifiers.iloc[index]['algorithm']
      name = classifiers.iloc[index]['name']
      save_model(algorithm,name)
      classifiers.iloc[index]['model_file'] = True
      print('pickle file'+name+' saved')
    except Exception as ex:
      print('fail on save pickle file: ',ex)

"""**Carregar Modelos Pickle salvos**"""

def classifiers_load():
  for index,classifier in classifiers.iterrows():
    try:
      name = classifiers.iloc[index]['name']
      model = load_model(name)
      classifiers.algorithm[index]= model
      classifiers_scores()
      print('Model ',index,' Loaded')
    except Exception as ex:
      print('Fail on load model file: ',ex)

"""**Setando o score do algoritmo em relação ao teste**"""

def classifiers_scores(score,index):
    try:
      classifiers.algorithm_score[index] = score
      classifiers.accuracy[index]=round(score*100,2),'%'
    except Exception as ex:
      print('Fail on pass scores',ex)

"""**Predict probabilities**"""

def roc_classifier(index):
  try:
    algorithm = classifiers.iloc[index]['algorithm']
    probs = algorithm.predict_proba(x_test)
    probs =probs[:,1]
    classifiers.probs[index]= probs
    
  except Exception as ex:
    print('fail on roc classification: ',ex)
    classifiers.probs[index]="not available for loss='hinge'"

def cauculate_auroc(index):
  try:
    probs = classifiers.iloc[index]['probs']
    auc = roc_auc_score(y_test,probs)
    classifiers.auc[index]=auc
  except Exception as ex:
    # print('Error on calculate auroc: ',index,ex)
    classifiers.auc[index] = "not available for loss='hinge'"

def plot_auroc():
  try:
   for index,classifier in classifiers.iterrows():
     fpr = classifiers.iloc[index]['fpr']
     tpr = classifiers.iloc[index]['tpr']
     auc = classifiers.iloc[index]['auc']
     name = classifiers.iloc[index]['name']
     plt.plot(fpr,tpr,linestyle='==',label = name % auc)

     plt.title('ROC plot')
     plt.xlabel('False Positive Rate')
     plt.ylabel('True Positive Rate')
     plt.legend()
     plt.show()
  except Exception as ex:
   print ('')

"""**Carregar algorítmos**"""

classifiers_fit()

classifiers

"""**Dividindo em Treino e Teste para os Algoritmos selecionados**

"""

y = data['FRAUDE']
X = data.drop(['FRAUDE'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)

y_train.value_counts()

smote = SMOTE(sampling_strategy="minority")
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

y_train_sm.value_counts()

smote_random_forest = RandomForestClassifier()
smote_random_forest.fit(X_train_sm,y_train_sm)

smote_rand_pred = smote_random_forest.predict(X_test)

from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score, roc_curve

print(confusion_matrix(y_test,smote_rand_pred))
print(classification_report(y_test,smote_rand_pred))

smote_Extra_Trees = ExtraTreesClassifier()
smote_Extra_Trees.fit(X_train_sm,y_train_sm)

smote_extra_pred = smote_Extra_Trees.predict(X_test)

print(confusion_matrix(y_test,smote_extra_pred))
print(classification_report(y_test,smote_extra_pred))

smote_K_Neighbors = KNeighborsClassifier()
smote_K_Neighbors.fit(X_train_sm,y_train_sm)

smote_kneighbors_pred = smote_Extra_Trees.predict(X_test)

print(confusion_matrix(y_test,smote_kneighbors_pred))
print(classification_report(y_test,smote_kneighbors_pred))

# describes info about train and test set
print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

lr = LogisticRegression()

lr.fit(X_train, y_train.ravel())

predictions = lr.predict(X_test)

print(classification_report(y_test, predictions))